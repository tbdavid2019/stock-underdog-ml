import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Input
from prophet import Prophet
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import datetime
import requests
from pymongo import MongoClient
import os
from dotenv import load_dotenv
from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
import mysql.connector
from mysql.connector import Error
from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame
# --- Crossâ€‘section æ¨¡å‹ç”¨ ---
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
import inspect, importlib, warnings
import pandas_ta as ta         # ç®—æŠ€è¡“æŒ‡æ¨™

# åŠ è¼‰ .env æ–‡ä»¶
load_dotenv()


# MySQL é…ç½®
use_mysql = os.getenv("USE_MYSQL", "false").lower() == "true"

***REMOVED*** è¨­ç½®
smtp_server = os.getenv("SMTP_SERVER")
port = int(os.getenv("SMTP_PORT"))
sender_email = os.getenv("SENDER_EMAIL")
password = os.getenv("EMAIL_PASSWORD")
to_emails = os.getenv("TO_EMAILS").split(",")

# Telegram è¨­ç½®
telegram_bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
telegram_channel_id = os.getenv("TELEGRAM_CHANNEL_ID")

# MongoDB é€£æ¥è¨­ç½®
mongo_uri = os.getenv("MONGO_URI")
db_name = "stock_predictions"

discord_webhook_url = os.getenv("DISCORD_WEBHOOK_URL")

# æ˜¯å¦ä½¿ç”¨ Transformer
use_transformer = os.getenv("USE_TRANSFORMER", "false").lower() == "true"
transformer_period = os.getenv("TRANSFORMER_PERIOD", "1y")  # é»˜èª 1 å¹´æ•¸æ“š

# æ˜¯å¦ä½¿ç”¨ Prophet
use_prophet = os.getenv("USE_PROPHET", "false").lower() == "true"

# æ˜¯å¦ä½¿ç”¨ chronos
use_chronos = os.getenv("USE_CHRONOS", "true").lower() == "true"
chronos_period = os.getenv("CHRONOS_PERIOD", "6mo")


class MySQLManager:
    def __init__(self):
        self.enabled = use_mysql
        if not self.enabled:
            print("MySQL åŠŸèƒ½æœªå•Ÿç”¨")
            return
        
        try:
            print("å˜—è©¦é€£æ¥ MySQL...")  # æ·»åŠ æ­¤è¡Œ
            print(f"Host: {os.getenv('MYSQL_HOST')}")  # æ·»åŠ æ­¤è¡Œ
            print(f"Database: {os.getenv('MYSQL_DATABASE')}")  # æ·»åŠ æ­¤è¡Œ
            self.connection = mysql.connector.connect(
                host=os.getenv("MYSQL_HOST"),
                user=os.getenv("MYSQL_USER"),
                password=os.getenv("MYSQL_PASSWORD"),
                database=os.getenv("MYSQL_DATABASE"),
                port=int(os.getenv("MYSQL_PORT", "3306"))
            )
            print("MySQL é€£æ¥æˆåŠŸ")
            self.create_prediction_table()
        except Error as e:
            print(f"MySQL é€£æ¥éŒ¯èª¤: {e}")
            self.connection = None
            self.enabled = False

    def create_prediction_table(self):
        if not self.enabled or not self.connection:
            return
        
        try:
            cursor = self.connection.cursor()
            
            # é¦–å…ˆæª¢æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_table_query = """
            SELECT COUNT(*)
            FROM information_schema.tables
            WHERE table_schema = %s
            AND table_name = 'stock_predictions'
            """
            cursor.execute(check_table_query, (os.getenv('MYSQL_DATABASE'),))
            table_exists = cursor.fetchone()[0] > 0

            if table_exists:
                print("stock_predictions è¡¨å·²å­˜åœ¨ï¼Œè·³éå‰µå»º")
            else:
                # å‰µå»ºè¡¨
                create_table_query = """
                CREATE TABLE IF NOT EXISTS stock_predictions (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    calculation_date DATE,
                    calculation_time TIME,
                    index_name VARCHAR(50),
                    stock_symbol VARCHAR(20),
                    current_price DECIMAL(10,2),
                    predicted_price DECIMAL(10,2),
                    potential DECIMAL(10,4),
                    prediction_method VARCHAR(20),
                    period_param VARCHAR(10),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                """
                cursor.execute(create_table_query)
                self.connection.commit()
                print("æˆåŠŸå‰µå»º stock_predictions è¡¨")

        except Error as e:
            print(f"æª¢æŸ¥/å‰µå»ºè¡¨æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            cursor.close()




    def save_predictions(self, index_name, predictions, method, period):
        if not self.enabled or not self.connection:
            return
        
        try:
            cursor = self.connection.cursor()
            current_date = datetime.datetime.now().date()
            current_time = datetime.datetime.now().time()
            
            insert_query = """
            INSERT INTO stock_predictions 
            (calculation_date, calculation_time, index_name, stock_symbol, 
             current_price, predicted_price, potential, prediction_method, period_param)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            for ticker, potential, current_price, predicted_price in predictions:
                data = (
                    current_date,
                    current_time,
                    index_name,
                    ticker,
                    float(current_price.iloc[0]) if isinstance(current_price, pd.Series) else float(current_price),
                    float(predicted_price.iloc[0]) if isinstance(predicted_price, pd.Series) else float(predicted_price),
                    float(potential.iloc[0]) if isinstance(potential, pd.Series) else float(potential),
                    method,
                    period
                )
                cursor.execute(insert_query, data)
            
            self.connection.commit()
            print(f"æˆåŠŸä¿å­˜ {len(predictions)} æ¢ {method} é æ¸¬çµæœåˆ° MySQL")
        
        except Error as e:
            print(f"ä¿å­˜åˆ° MySQL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            cursor.close()

    def close(self):
        if self.connection and self.connection.is_connected():
            self.connection.close()
            print("MySQL é€£æ¥å·²é—œé–‰")

def save_to_mongodb(index_name, stock_predictions):
    """
    å°‡è‚¡ç¥¨é æ¸¬çµæœå­˜å…¥ MongoDB
    :param index_name: æŒ‡æ•¸åç¨±
    :param stock_predictions: é æ¸¬çµæœ (dict)
    """
    try:
        client = MongoClient(mongo_uri)
        db = client[db_name]
        collection = db["predictions"]

        # è¨­ç½®è¦å¯«å…¥çš„æ–‡ä»¶æ ¼å¼
        record = {
            "index": index_name,
            "timestamp": datetime.datetime.now(),
            "predictions": stock_predictions
        }

        # å¯«å…¥ MongoDB
        collection.insert_one(record)
        print(f"æˆåŠŸå°‡ {index_name} çµæœå¯«å…¥ MongoDB")
    except Exception as e:
        print(f"âš ï¸ å¯«å…¥ MongoDB å¤±æ•—: {str(e)}")
    finally:
        client.close()

# Stock index mappings

def get_tw0050_stocks():
    response = requests.get('https://answerbook.david888.com/TW0050')
    data = response.json()
    
    # å–å¾—è‚¡ç¥¨ä»£ç¢¼ä¸¦åŠ ä¸Š .TW
    stocks = [f"{code}.TW" for code in data['TW0050'].keys()]
    
    # å¦‚æœéœ€è¦æ’åºçš„è©±å¯ä»¥åŠ ä¸Š sort()
    #stocks.sort()
    
    return stocks


def get_tw0051_stocks():
    response = requests.get('https://answerbook.david888.com/TW0051')
    data = response.json()
    
    # å–å¾—è‚¡ç¥¨ä»£ç¢¼ä¸¦åŠ ä¸Š .TW
    stocks = [f"{code}.TW" for code in data['TW0051'].keys()]
    
    # å¦‚æœéœ€è¦æ’åºçš„è©±å¯ä»¥åŠ ä¸Š sort()
    # stocks.sort()
    
    return stocks


def get_sp500_stocks(limit=100):
    response = requests.get('https://answerbook.david888.com/SP500')
    data = response.json()
    
    # å–å¾—è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨ä¸¦é™åˆ¶æ•¸é‡
    stocks = list(data['SP500'].keys())[:limit]
    
    # ä¿®æ­£ç‰¹æ®Šè‚¡ç¥¨ä»£ç¢¼æ ¼å¼ï¼Œä¾‹å¦‚å°‡ BRK.B è½‰æ›ç‚º BRK-B ä»¥é©æ‡‰ yfinance
    for i, stock in enumerate(stocks):
        if stock == "BRK.B":
            stocks[i] = "BRK-B"
    
    return stocks
    

# Function to fetch NASDAQ component stocks
def get_nasdaq_stocks():
# Function to fetch Philadelphia Semiconductor Index component stocks

    response = requests.get('https://answerbook.david888.com/nasdaq100')
    data = response.json()
    
    # å–å¾—è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨ä¸¦é™åˆ¶æ•¸é‡
    stocks = list(data['nasdaq100'].keys())
    
    return stocks


def get_sox_stocks():
    return [
        "NVDA", "AVGO", "GFS", "CRUS", "ON", "ASML", "QCOM", "SWKS", "MPWR", "ADI",
        "TSM", "AMD", "TXN", "QRVO", "AMKR", "MU", "ARM", "NXPI", "TER", "ENTG",
        "LSCC", "COHR", "ONTO", "MTSI", "KLAC", "LRCX", "MRVL", "AMAT", "INTC", "MCHP"
    ]

# Function to fetch Dow Jones Industrial Average component stocks
def get_dji_stocks():

    response = requests.get('https://answerbook.david888.com/dowjones')
    data = response.json()
    
    # å–å¾—è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨ä¸¦é™åˆ¶æ•¸é‡
    stocks = list(data['dowjones'].keys())
    
    return stocks


# ç²å–è‚¡ç¥¨æ•¸æ“š
def get_stock_data(ticker, period):
    try:
        print(f"æ­£åœ¨ç²å– {ticker} çš„æ•¸æ“š...")
        data = yf.download(ticker, period=period)
        print(f"ç²å–åˆ° {len(data)} æ¢äº¤æ˜“æ—¥æ•¸æ“š")
        return data
    except Exception as e:
        print(f"ç²å– {ticker} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return pd.DataFrame()


def prepare_chronos_data(data):
    df = data.reset_index()
    formatted_df = pd.DataFrame({
        'item_id': ['stock'] * len(df),
        'timestamp': pd.to_datetime(df['Date']),
        'target': df['Close'].astype('float32').values.ravel()
    })
    formatted_df = formatted_df.sort_values('timestamp')
    try:
        ts_df = TimeSeriesDataFrame.from_data_frame(
            formatted_df,
            id_column='item_id',
            timestamp_column='timestamp'
        )
        return ts_df
    except Exception as e:
        print(f"Error creating TimeSeriesDataFrame: {str(e)}")
        raise

def prepare_data(data, time_step=60):
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data[['Open', 'High', 'Low', 'Close',   'Volume']])
    X, y = [], []
    for i in range(time_step, len(scaled_data)):
        X.append(scaled_data[i - time_step:i])  # ç¢ºä¿ X æ˜¯ (time_step, features)
        y.append(scaled_data[i, 3])  # é æ¸¬ Close åƒ¹        
    X, y = np.array(X), np.array(y).reshape(-1, 1)  # y çš„å½¢ç‹€æ‡‰ç‚º (samples, 1)
    return X, y, scaler


def train_lstm_model(X_train, y_train):
    # ä½¿ç”¨å¤šå€‹ç‰¹å¾µä½œç‚ºè¼¸å…¥
    model = Sequential([
        Input(shape=(X_train.shape[1], X_train.shape[2])),  # ä¿®æ”¹è¼¸å…¥å½¢ç‹€
        LSTM(units=50, return_sequences=True),
        Dropout(0.2),
        LSTM(units=50, return_sequences=False),
        Dropout(0.2),
        Dense(units=1)  # é æ¸¬ Close
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    model.fit(X_train, y_train, epochs=10, batch_size=32)
    return model




def predict_stock(model, data, scaler, time_step=60):
    # ä½¿ç”¨å¤šå€‹ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–
    scaled_data = scaler.transform(data[['Open', 'High', 'Low', 'Close',   'Volume']])

    # æº–å‚™æ¸¬è©¦é›†
    X_test = [scaled_data[i-time_step:i] for i in range(time_step, len(scaled_data))]
    X_test = np.array(X_test)

    # LSTM é æ¸¬
    predicted_prices = model.predict(X_test)

    # åæ¨™æº–åŒ–ï¼Œåƒ…å° `Close` ç‰¹å¾µé€²è¡Œåæ¨™æº–åŒ–
    close_index = 3  # 'Close' çš„ç´¢å¼•
    predicted_close_prices = scaler.inverse_transform(
        np.concatenate([
            np.zeros((predicted_prices.shape[0], close_index)),  # å¡«å……å¤šé¤˜çš„ç¶­åº¦
            predicted_prices,  # æ’å…¥é æ¸¬çš„ Close
            np.zeros((predicted_prices.shape[0], scaled_data.shape[1] - close_index - 1))  # å¡«å……å¤šé¤˜çš„ç¶­åº¦
        ], axis=1)
    )[:, close_index]  # åªå–åæ¨™æº–åŒ–å¾Œçš„ Close
    return predicted_close_prices


# Prophet é æ¸¬è‚¡ç¥¨
def train_prophet_model(data):
    # é‡ç½®ç´¢å¼•å¹¶å‡†å¤‡æ•°æ®
    df = data.reset_index()[['Date', 'Close']]
    df.columns = ['ds', 'y']  # Prophet è¦æ±‚çš„æ ¼å¼
    df = df.dropna()  # ç§»é™¤ç¼ºå¤±å€¼

    # ç¡®ä¿æ²¡æœ‰è´Ÿå€¼
    if (df['y'] < 0).any():
        raise ValueError("å‘ç°è´Ÿå€¼ï¼Œæ— æ³•è®­ç»ƒ Prophet æ¨¡å‹")

    # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
    if len(df) < 30:  # è‡³å°‘éœ€è¦ 30 æ¡æ•°æ®
        raise ValueError("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒ Prophet æ¨¡å‹")

    # åˆå§‹åŒ– Prophet æ¨¡å‹ 
    model = Prophet(yearly_seasonality=True, daily_seasonality=True, changepoint_prior_scale=0.1)
    model.fit(df)
    return model

# Prophet é¢„æµ‹è‚¡ç¥¨
def predict_with_prophet(model, data, prediction_days=3):
    """
    ä½¿ç”¨ Prophet é æ¸¬è¿‘æœŸè‚¡ç¥¨åƒ¹æ ¼
    :param model: è¨“ç·´å¥½çš„ Prophet æ¨¡å‹
    :param data: åŸå§‹è‚¡ç¥¨æ•¸æ“šï¼ˆåŒ…å« Closeï¼‰
    :param prediction_days: é æ¸¬å¤©æ•¸ï¼Œé»˜èªç‚º 1ï¼ˆéš”æ—¥ï¼‰
    :return: é æ¸¬çµæœçš„ DataFrame
    """
    # è·å–æœ€æ–°çš„ Close å€¼
    last_close = data['Close'].values[-1]

    # åˆ›å»ºæœªæ¥æ—¥æœŸ
    future = model.make_future_dataframe(periods=prediction_days)

    # é¢„æµ‹æœªæ¥æ•°æ®
    forecast = model.predict(future)

    # è¨­ç½®åˆç†çš„ä¸Šä¸‹é™ï¼Œé¿å…é æ¸¬å€¼éæ–¼èª‡å¼µ
    lower_bound = last_close * 0.8
    upper_bound = last_close * 1.2
    forecast['yhat'] = forecast['yhat'].apply(lambda x: min(max(x, lower_bound), upper_bound))

    # è¿”å›æœ€è¿‘çš„é æ¸¬å€¼
    return forecast.tail(prediction_days)[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]


# æ§‹å»º Transformer æ¨¡å‹
def build_transformer_model(input_shape):
    inputs = Input(shape=input_shape)
    
    # Transformer Encoder Layer
    attention = MultiHeadAttention(num_heads=4, key_dim=input_shape[-1])(inputs, inputs)
    attention = Dropout(0.1)(attention)
    attention = Add()([inputs, attention])  # æ®˜å·®é€£æ¥
    attention = LayerNormalization(epsilon=1e-6)(attention)

    # Feed Forward Layer
    feed_forward = Dense(64, activation="relu")(attention)
    feed_forward = Dropout(0.1)(feed_forward)
    outputs = Dense(1)(feed_forward)  # é æ¸¬ Close åƒ¹
    
    model = Model(inputs, outputs)
    model.compile(optimizer=Adam(learning_rate=0.001), loss="mean_squared_error")
    return model

# è¨“ç·´ Transformer æ¨¡å‹
def train_transformer_model(X_train, y_train, input_shape):
    model = build_transformer_model(input_shape)
    model.fit(X_train, y_train, epochs=10, batch_size=32)
    return model

# Transformer é æ¸¬
def predict_transformer(model, data, scaler, time_step=60):
    # ä½¿ç”¨å¤šå€‹ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–
    scaled_data = scaler.transform(data[['Open', 'High', 'Low', 'Close',   'Volume']])

    # æº–å‚™æ¸¬è©¦é›†
    X_test = [scaled_data[i-time_step:i] for i in range(time_step, len(scaled_data))]
    X_test = np.array(X_test)

    # æ‰“å° X_test çš„å½¢ç‹€
    print(f"X_test shape: {X_test.shape}")

    # Transformer é æ¸¬
    predicted_prices = model.predict(X_test)

    # ä¿®æ­£ predicted_prices çš„å½¢ç‹€
    if len(predicted_prices.shape) > 2:
        predicted_prices = predicted_prices[:, -1, 0]  # å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„é æ¸¬å€¼

    # æ‰“å° predicted_prices çš„å½¢ç‹€
    print(f"predicted_prices shape after reshape: {predicted_prices.shape}")

    # åæ¨™æº–åŒ–ï¼Œåªå° Close ç‰¹å¾µé€²è¡Œ
    close_index = 3  # Close ç‰¹å¾µåœ¨ scaled_data ä¸­çš„ç´¢å¼•

    # æ§‹å»ºå®Œæ•´çš„æ•¸æ“šçµæ§‹ï¼Œå¡«å……åˆ°èˆ‡ scaled_data ä¸€è‡´çš„å½¢ç‹€
    full_predictions = np.zeros((predicted_prices.shape[0], scaled_data.shape[1]))
    full_predictions[:, close_index] = predicted_prices  # æ’å…¥ Close é æ¸¬å€¼

    # ä½¿ç”¨ scaler é€²è¡Œåæ¨™æº–åŒ–
    predicted_close_prices = scaler.inverse_transform(full_predictions)[:, close_index]
    return predicted_close_prices


# =========  Crossâ€‘section utilities  =========
CROSS_MODELS = [
    ('qlib.contrib.model.pytorch_tabnet', ['TabNet', 'TabnetModel'])
    # ('qlib.contrib.model.pytorch_sfm',    ['SFM', 'SFMModel']),
    # ('qlib.contrib.model.pytorch_add',    ['ADDModel'])
]


def train_cross_loop(model_cls, X, y, epochs, device="cpu"):
    """
    é€šç”¨æ©«æ–·é¢è¨“ç·´è¿´åœˆï¼Œæ”¯æ´ TabNet / SFM / ADDModel
    æœƒæ ¹æ“šæ¨¡å‹çš„ __init__ åƒæ•¸è‡ªå‹•æ±ºå®šè¦çµ¦å“ªäº› kwargs
    """
    import inspect
    sig = inspect.signature(model_cls.__init__)
    param_names = sig.parameters

    kw = {}
    # --- ç‰¹å¾µè¼¸å…¥ç¶­åº¦ ---
    if 'd_feat'      in param_names: kw['d_feat']      = X.shape[1]
    if 'feature_dim' in param_names: kw['feature_dim'] = X.shape[1]
    if 'input_dim'   in param_names: kw['input_dim']   = X.shape[1]
    if 'field_dim'   in param_names: kw['field_dim']   = X.shape[1]

    # --- è¼¸å‡ºç¶­åº¦ ---
    if 'output_dim'  in param_names: kw['output_dim']  = 1
    if 'target_dim'  in param_names: kw['target_dim']  = 1
    if 'embed_dim'   in param_names: kw['embed_dim']   = 16   # ä¾‹å¦‚ SFM ç”¨

    model = model_cls(**kw)
    net   = model.model if hasattr(model, 'model') else model

    # æœ‰äº›æ¨¡å‹æœ¬èº«ä¸æ”¯æ´ .to()
    if hasattr(net, 'to'):
        net.to(device)

    ds = DataLoader(TensorDataset(X.to(device), y.to(device)),
                    batch_size=512, shuffle=True)
    opt = torch.optim.Adam(net.parameters(), lr=1e-3)
    loss_fn = nn.MSELoss()

    net.train()
    for _ in range(epochs):
        for xb, yb in ds:
            opt.zero_grad()
            out = net(xb)
            out = out[0] if isinstance(out, tuple) else out
            loss_fn(out.squeeze(), yb).backward()
            opt.step()

    net.eval()
    with torch.no_grad():
        preds = net(X.to(device))
        preds = preds[0] if isinstance(preds, tuple) else preds
        preds = preds.squeeze().cpu().numpy()

    return preds
    
def add_indicators(df):
    """çµ¦å–®æ”¯è‚¡ç¥¨ DataFrame åŠ  MA5 / MA10 / RSI14"""
    df.ta.strategy(ta.Strategy(
        name="ma_rsi",
        ta=[ {"kind":"sma","length":5},
             {"kind":"sma","length":10},
             {"kind":"rsi","length":14} ]))
    df.rename(columns={'SMA_5':'ma5','SMA_10':'ma10','RSI_14':'rsi14'}, inplace=True)
    return df

def download_many(tickers, period):
    """ä¸€æ¬¡æŠ“å¤šè‚¡ç¥¨æ—¥ç·šï¼Œå›å‚³æ‰å¹³ DF"""
    data = yf.download(" ".join(tickers), period=period,
                       group_by='ticker', auto_adjust=True, threads=True)
    frames=[]
    if isinstance(data.columns, pd.MultiIndex):
        for tic in tickers:
            sub = data[tic].copy()
            sub.columns = ['Open','High','Low','Close','Adj Close','Volume'][:len(sub.columns)]
            sub = add_indicators(sub); sub['Ticker']=tic; frames.append(sub)
    else:  # åªæŠ“åˆ° 1 æª”
        data = add_indicators(data); data['Ticker']=tickers[0]; frames.append(data)

    df = pd.concat(frames).reset_index().rename(columns={'index':'Date'})

    # âœ ä¿è­‰å…­å¤§æ¬„éƒ½åœ¨ï¼Œè‹¥ç¼ºå°±è£œ 0
    base_cols = ['Open','High','Low','Close','Adj Close','Volume']
    for c in base_cols:
        if c not in df.columns:
            df[c] = 0.0    # å¡« 0ï¼ˆæˆ–ç”¨ np.nanï¼‰

    cols = ['Date','Ticker','Open','High','Low','Close','Volume','ma5','ma10','rsi14']
    df['Date'] = pd.to_datetime(df['Date'])  # ç¢ºä¿ Date æ¬„ä½ç‚º datetime å‹æ…‹
    df = df.reset_index(drop=True)           # ç¢ºä¿æ²’æœ‰ MultiIndex
    return df[cols].dropna().sort_values(['Date','Ticker'])



def build_cross_xy(df):
    """
    ç”¢ç”Ÿ tabular ç‰¹å¾µèˆ‡æ¨™ç±¤
    æ¨™ç±¤ = ä¸‹ä¸€æ—¥åƒ¹æ ¼å·®ç™¾åˆ†æ¯” = (Close(t+1) âˆ’ Close(t)) / Close(t)
    """
    df = df.copy()
    # å»ºç«‹ç™¾åˆ†æ¯”æ¨™ç±¤
    df['pct_ret1'] = (
        df.groupby('Ticker')['Close'].transform(lambda x: x.shift(-1)) - df['Close']
    ) / df['Close']

    df = df.dropna()                 # ç§»é™¤æœ€å¾Œä¸€å¤©ç„¡æ¨™ç±¤è³‡æ–™
    feats = ['Open','High','Low','Close','Volume','ma5','ma10','rsi14']

    X = torch.tensor(df[feats].values, dtype=torch.float32)
    y = torch.tensor(df['pct_ret1'].values, dtype=torch.float32)
    meta = df[['Date','Ticker']].reset_index(drop=True)
    return X, y, meta

def import_model(mod_path, cls_list):
    try:
        m = importlib.import_module(mod_path)
        for c in cls_list:
            if hasattr(m, c): return getattr(m, c)
    except ImportError: pass
    return None

# -------------------------------------------
# æ­£ç¢ºçš„ TabNet handâ€‘loop  (with priors tensor)
# -------------------------------------------
from qlib.contrib.model.pytorch_tabnet import TabNet

def train_tabnet(X, y, epochs=150, device="cpu"):
    inp = X.shape[1]
    net = TabNet(inp_dim=inp, out_dim=1).to(device)

    loader = DataLoader(TensorDataset(X.to(device), y.to(device)),
                        batch_size=512, shuffle=True)
    opt, loss_fn = torch.optim.Adam(net.parameters(), lr=1e-3), nn.MSELoss()

    net.train()
    for _ in range(epochs):
        for xb, yb in loader:
            pri = torch.ones(xb.size(0), inp, device=device)  # å…¨ 1 é®ç½©
            opt.zero_grad()
            raw = net(xb, priors=pri)            # â† å¯èƒ½ tuple
            out = raw[0] if isinstance(raw, tuple) else raw
            loss_fn(out.squeeze(), yb).backward()
            opt.step()

    net.eval()
    with torch.no_grad():
        pri_all = torch.ones(X.size(0), inp, device=device)
        raw_all = net(X.to(device), priors=pri_all)
        preds = (raw_all[0] if isinstance(raw_all, tuple) else raw_all
                ).squeeze().cpu().numpy()
    return preds
# =========  Crossâ€‘section utilities  =========

# ç™¼é€é›»å­éƒµä»¶
def send_email(subject, body, to_emails):
    msg = MIMEMultipart()
    msg['From'] = sender_email
    msg['To'] = ", ".join(to_emails)
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))
    server = smtplib.SMTP_SSL(smtp_server, port)
    server.login(sender_email, password)
    server.sendmail(sender_email, to_emails, msg.as_string())
    server.quit()

# ç™¼é€ Telegram æ¶ˆæ¯
def send_to_telegram(message):
    url = f"https://api.telegram.org/bot{telegram_bot_token}/sendMessage"
    payload = {"chat_id": telegram_channel_id, "text": message, "parse_mode": "HTML"}
    response = requests.post(url, json=payload)
    if response.status_code != 200:
        print(f"Telegram ç™¼é€å¤±æ•—: {response.text}")


# ç™¼é€ Discord æ¶ˆæ¯
def send_to_discord(message):
    try:
        payload = {
            "content": message
        }
        headers = {
            "Content-Type": "application/json"
        }
        response = requests.post(discord_webhook_url, json=payload, headers=headers)  # ä½¿ç”¨å…¨åŸŸè®Šæ•¸
        if response.status_code == 204:
            print("è¨Šæ¯å·²æˆåŠŸå‚³é€åˆ° Discord é »é“ã€‚")
        else:
            print(f"å‚³é€è¨Šæ¯åˆ° Discord æ™‚ç™¼ç”ŸéŒ¯èª¤: {response.status_code}, {response.text}")
    except Exception as e:
        print(f"å‚³é€è¨Šæ¯åˆ° Discord æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

def send_results(index_name, stock_predictions):
    calculation_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"ç™¼é€çµæœ: {index_name}")

    # å°‡çµæœå­˜å…¥ MongoDBï¼ˆå¯é¸åŠŸèƒ½ï¼‰
    save_to_mongodb(index_name, stock_predictions)

    # çµ„è£ Email å…§å®¹
    email_subject = f"æ¯æ—¥æ½›åŠ›è‚¡åˆ†æDAVID888 - {index_name} - é‹ç®—æ™‚é–“: {calculation_time}"
    email_body = f"é‹ç®—æ—¥æœŸå’Œæ™‚é–“: {calculation_time}\n\næŒ‡æ•¸: {index_name}\n"
    for key, predictions in stock_predictions.items():
        email_body += f"\n{key}:\n"
        for stock in predictions:
            email_body += f"è‚¡ç¥¨: {stock[0]}, æ½›åŠ›: {stock[1]:.2%}, ç¾åƒ¹: {stock[2]:.2f}, é æ¸¬åƒ¹: {stock[3]:.2f}\n"
    send_email(email_subject, email_body, to_emails)

    # çµ„è£ Telegram å…§å®¹
    telegram_message = f"<b>æ¯æ—¥æ½›åŠ›è‚¡åˆ†æ</b>\né‹ç®—æ—¥æœŸå’Œæ™‚é–“: <b>{calculation_time}</b>\n\næŒ‡æ•¸: <b>{index_name}</b>\n"
    for key, predictions in stock_predictions.items():
        telegram_message += f"<b>{key}:</b>\n"
        for stock in predictions:
            telegram_message += f"è‚¡ç¥¨: {stock[0]}, æ½›åŠ›: {stock[1]:.2%}, ç¾åƒ¹: {stock[2]:.2f}, é æ¸¬åƒ¹: {stock[3]:.2f}\n"
    send_to_telegram(telegram_message)

    # çµ„è£ Discord å…§å®¹
    discord_message = f"**æ¯æ—¥æ½›åŠ›è‚¡åˆ†æ**\né‹ç®—æ—¥æœŸå’Œæ™‚é–“: **{calculation_time}**\n\næŒ‡æ•¸: **{index_name}**\n"
    for key, predictions in stock_predictions.items():
        discord_message += f"**{key}:**\n"
        for stock in predictions:
            discord_message += f"è‚¡ç¥¨: {stock[0]}, æ½›åŠ›: {stock[1]:.2%}, ç¾åƒ¹: {stock[2]:.2f}, é æ¸¬åƒ¹: {stock[3]:.2f}\n"
    print("[DEBUG] discord_message çµ„è£å…§å®¹ï¼š")
    print(discord_message)
    send_to_discord(discord_message)  # ä¸å†å‚³å…¥ webhook_url


# -------------------------------------------------------
# è‚¡ç¥¨åˆ†æå‡½æ•¸ï¼šå…ˆåŸ·è¡Œæ©«æ–·é¢æ¨¡å‹ï¼Œå†åŸ·è¡Œæ™‚é–“åºåˆ—æ¨¡å‹
# -------------------------------------------------------
def get_top_and_bottom_10_potential_stocks(period, selected_indices, mysql_manager=None):
    """
    ä¾æ‰€é¸æŒ‡æ•¸ï¼Œå›å‚³å„æ¨¡å‹æ½›åŠ›æ’è¡Œæ¦œï¼ˆå‰ / å¾Œ 10ï¼‰
    çµæ§‹ç¯„ä¾‹ï¼š
    {
        "å°ç£50": {
            "ğŸ¥‡ å‰åå LSTM ğŸ§ ":    [ (ticker, pot, curr, pred), ... ],
            "ğŸ“‰ å¾Œåå LSTM ğŸ§ ":    [ ... ],
            ...
            "ğŸš€ å‰åå TabNet":     [ ... ],
            "â›” å¾Œåå TabNet":     [ ... ]
        }, ...
    }
    """
    results = {}
    # stock_predictions = {}  # ç§»é™¤é€™è¡Œï¼Œæ”¹åˆ°æ¯å€‹æŒ‡æ•¸è™•ç†æ™‚åˆå§‹åŒ–

    # --- æŒ‡æ•¸ â†’ è‚¡ç¥¨æ¸…å–® ---------------------------------
    index_stock_map = {
        "å°ç£50":      get_tw0050_stocks(),
        "å°ç£ä¸­å‹100": get_tw0051_stocks(),
        "SP500":       get_sp500_stocks(),
        "NASDAQ":      get_nasdaq_stocks(),
        "è²»åŸåŠå°é«”":   get_sox_stocks(),
        "é“ç“Š":        get_dji_stocks(),
    }

    # --- å…¨åŸŸè¨­å®š ---------------------------------------
    use_cross    = os.getenv("USE_CROSS", "false").lower() == "true"
    cross_period = os.getenv("CROSS_PERIOD", "6mo")
    cross_epochs = int(os.getenv("CROSS_EPOCHS", "150"))
    device       = "cuda" if torch.cuda.is_available() else "cpu"

    # --- æ™‚é–“åºåˆ—æ¨¡å‹è¨­å®š ---------------------------------
    use_transformer = os.getenv("USE_TRANSFORMER", "false").lower() == "true"
    use_prophet = os.getenv("USE_PROPHET", "false").lower() == "true"
    use_chronos = os.getenv("USE_CHRONOS", "true").lower() == "true"

    for index_name, stock_list in index_stock_map.items():
        stock_predictions = {}  # æ¯å€‹æŒ‡æ•¸éƒ½é‡æ–°åˆå§‹åŒ–
        if index_name not in selected_indices:
            continue
        print(f"\n=== è™•ç†æŒ‡æ•¸: {index_name} ===")

        # -------- åºåˆ—æ¨¡å‹å®¹å™¨ --------
        lstm_preds, prophet_preds = [], []
        transformer_preds, chronos_preds = [], []

        # ======== å…ˆè·‘æ©«æ–·é¢æ¨¡å‹ï¼ˆCross Sectional Modelsï¼‰ ========
        if use_cross:
            try:
                raw_df = download_many(stock_list, cross_period)
                Xc, yc, meta_c = build_cross_xy(raw_df)

                # ä¿®æ­£ mask_last = Series vs Series éŒ¯èª¤
                max_date = pd.Timestamp(meta_c['Date'].max())
                print(f"[LOG] max_date: {max_date}, type: {type(max_date)}")
                print(f"[LOG] meta_c['Date'] head: {meta_c['Date'].head()}, dtype: {meta_c['Date'].dtype}")
                mask_last = meta_c['Date'].values == max_date
                print(f"[LOG] mask_last: {mask_last}, shape: {mask_last.shape}, type: {type(mask_last)}")
                meta_last = meta_c[mask_last].reset_index(drop=True)
                print(f"[LOG] meta_last shape: {meta_last.shape}, columns: {meta_last.columns}")

                latest_close = (
                    raw_df[raw_df['Date'] == max_date]
                    .groupby('Ticker')['Close']
                    .first()
                )
                print(f"[LOG] latest_close index: {latest_close.index}, type: {type(latest_close)}")

                # åŸ·è¡Œ Cross æ¨¡å‹ï¼ˆTabNetï¼ŒSFMï¼ŒADDModelï¼‰
                for m_path, cls_list in CROSS_MODELS:
                    ModelClass = import_model(m_path, cls_list)
                    if ModelClass is None:
                        continue
                    print(f"ğŸ” Cross è¨“ç·´ {ModelClass.__name__} â€¦")
                    try:
                        if ModelClass.__name__ == "TabNet":
                            preds_all = train_tabnet(Xc, yc, epochs=cross_epochs, device=device)
                        else:
                            preds_all = train_cross_loop(ModelClass, Xc, yc, cross_epochs, device)
                        print(f"[LOG] preds_all shape: {getattr(preds_all, 'shape', None)}, type: {type(preds_all)}")
                        preds_last = preds_all[mask_last]
                        print(f"[LOG] preds_last shape: {getattr(preds_last, 'shape', None)}, type: {type(preds_last)}")

                        # çµ„ TabNet / SFM / ADDModel çµæœ
                        records = [
                            (
                                tic,
                                p,                               # é æ¸¬æ½›åŠ›
                                float(latest_close[tic]),        # ç¾åƒ¹
                                float(latest_close[tic] * (1+p)) # é æ¸¬åƒ¹
                            )
                            for tic, p in zip(meta_last['Ticker'], preds_last)
                        ]
                        print(f"[LOG] records sample: {records[:3]}")

                        # å¯« MySQL
                        if mysql_manager and mysql_manager.enabled:
                            mysql_manager.save_predictions(index_name, records, ModelClass.__name__, cross_period)

                        # æ’è¡Œæ¦œ
                        stock_predictions.update({
                            f"ğŸš€ å‰åå {ModelClass.__name__}": sorted(records, key=lambda x:x[1], reverse=True)[:10],
                            f"â›” å¾Œåå {ModelClass.__name__}": sorted(records, key=lambda x:x[1])[:10],
                        })
                        print(f"[DEBUG] stock_predictions keys after update: {list(stock_predictions.keys())}")
                        print(f"[DEBUG] stock_predictions lens after update: {[len(v) for v in stock_predictions.values()]}")

                    except Exception as e:
                        print(f"{ModelClass.__name__} å¤±æ•—: {e}")
                        continue

            except Exception as e:
                print(f"Crossâ€‘section æµç¨‹éŒ¯èª¤: {e}")

        # ======== è·‘æ™‚é–“åºåˆ—æ¨¡å‹ ========
        for tic in stock_list:
            data = get_stock_data(tic, period)
            if len(data) < 60:
                continue

            # ----- LSTM -----
            try:
                X, y, scaler = prepare_data(data)
                lstm_model = train_lstm_model(X, y)
                lstm_series = predict_stock(lstm_model, data, scaler)
                print(f"[DEBUG][LSTM] lstm_series type: {type(lstm_series)}, shape: {getattr(lstm_series, 'shape', None)}")
                print(f"[DEBUG][LSTM] data['Close'] type: {type(data['Close'])}, shape: {getattr(data['Close'], 'shape', None)}")
                cur  = float(data['Close'].iloc[-1, 0]) if isinstance(data['Close'], pd.DataFrame) else float(data['Close'].iloc[-1])
                pred = float(lstm_series.max())
                pot  = (pred - cur) / cur
                print(f"[DEBUG][LSTM] cur: {cur}, pred: {pred}, pot: {pot}")
                lstm_preds.append((tic, pot, cur, pred))
            except Exception as e:
                print(f"LSTM å¤±æ•— {tic}: {e}")

            # ----- Transformer -----
            if use_transformer:
                try:
                    tf_data = get_stock_data(tic, transformer_period)
                    X_tf, y_tf, tf_scaler = prepare_data(tf_data)
                    tf_shape = (X_tf.shape[1], X_tf.shape[2])
                    tf_model = train_transformer_model(X_tf, y_tf, tf_shape)
                    tf_series = predict_transformer(tf_model, tf_data, tf_scaler)
                    cur  = tf_data['Close'].iloc[-1]
                    pred = float(tf_series.max())
                    pot  = (pred - cur) / cur
                    transformer_preds.append((tic, pot, cur, pred))
                except Exception as e:
                    print(f"Transformer å¤±æ•— {tic}: {e}")

            # ----- Prophet -----
            if use_prophet:
                try:
                    p_model = train_prophet_model(data)
                    p_fore  = predict_with_prophet(p_model, data)
                    cur  = data['Close'].iloc[-1]
                    pred = float(p_fore['yhat'].max())
                    pot  = (pred - cur) / cur
                    prophet_preds.append((tic, pot, cur, pred))
                except Exception as e:
                    print(f"Prophet å¤±æ•— {tic}: {e}")

            # ----- Chronosâ€‘Bolt -----
            if use_chronos:
                try:
                    ch_data = get_stock_data(tic, chronos_period)
                    if len(ch_data) < 60:
                        continue
                    ts_df = prepare_chronos_data(ch_data)
                    predictor = TimeSeriesPredictor(
                        prediction_length=10, freq="D", target="target")
                    predictor.fit(ts_df, hyperparameters={
                        "Chronos": {"model_path": "autogluon/chronos-bolt-base"}
                    })
                    preds = predictor.predict(ts_df)
                    cur  = float(ch_data['Close'].iloc[-1])
                    pred = float(preds.values.max())
                    pot  = (pred - cur) / cur
                    chronos_preds.append((tic, pot, cur, pred))
                except Exception as e:
                    print(f"Chronos å¤±æ•— {tic}: {e}")

        # --- MySQLï¼šæ™‚é–“åºåˆ—æ¨¡å‹ ---------------------------
        if mysql_manager and mysql_manager.enabled:
            if lstm_preds:
                mysql_manager.save_predictions(index_name, lstm_preds, "LSTM", period)
            if use_prophet and prophet_preds:
                mysql_manager.save_predictions(index_name, prophet_preds, "Prophet", period)
            if use_transformer and transformer_preds:
                mysql_manager.save_predictions(index_name, transformer_preds, "Transformer", period)
            if use_chronos and chronos_preds:
                mysql_manager.save_predictions(index_name, chronos_preds, "Chronos-Bolt", chronos_period)

        # --- çµ„æ’è¡Œæ¦œï¼ˆæ™‚é–“åºåˆ—ï¼‰ -------------------------
        stock_predictions = stock_predictions if 'stock_predictions' in locals() else {}

        stock_predictions.update({
            "ğŸ¥‡ å‰åå LSTM ğŸ§ ": sorted(lstm_preds, key=lambda x: x[1], reverse=True)[:10],
            "ğŸ“‰ å¾Œåå LSTM ğŸ§ ": sorted(lstm_preds, key=lambda x: x[1])[:10],
        })
        if use_prophet and prophet_preds:
            stock_predictions.update({
                "ğŸš€ å‰åå Prophet ğŸ”®": sorted(prophet_preds, key=lambda x: x[1], reverse=True)[:10],
                "â›” å¾Œåå Prophet ğŸ”®": sorted(prophet_preds, key=lambda x: x[1])[:10],
            })
        if use_transformer and transformer_preds:
            stock_predictions.update({
                "ğŸš€ å‰åå Transformer ğŸ”„": sorted(transformer_preds, key=lambda x: x[1], reverse=True)[:10],
                "â›” å¾Œåå Transformer ğŸ”„": sorted(transformer_preds, key=lambda x: x[1])[:10],
            })
        if use_chronos and chronos_preds:
            stock_predictions.update({
                "ğŸš€ å‰åå Chronos-Bolt âš¡": sorted(chronos_preds, key=lambda x: x[1], reverse=True)[:10],
                "â›” å¾Œåå Chronos-Bolt âš¡": sorted(chronos_preds, key=lambda x: x[1])[:10],
            })

        # -------- æ”¶å°¾ --------
        if stock_predictions:
            results[index_name] = stock_predictions

    return results


# ä¸»å‡½æ•¸
def main():
    try:
        # åˆå§‹åŒ– MySQL ç®¡ç†å™¨
        mysql_manager = MySQLManager() if use_mysql else None

        calculation_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        period = "6mo"
        #selected_indices = ["å°ç£50", "å°ç£ä¸­å‹100", "SP500"]
        selected_indices = ["SP500"]

        print("è¨ˆç®—æ½›åŠ›è‚¡...")
        analysis_results = get_top_and_bottom_10_potential_stocks(period, selected_indices, mysql_manager)

        # åˆ†é–‹è™•ç†æ¯å€‹æŒ‡æ•¸çš„çµæœ
        for index_name, stock_predictions in analysis_results.items():
            print(f"è™•ç†ä¸¦ç™¼é€çµæœ: {index_name}")
            send_results(index_name, stock_predictions)

    except Exception as e:
        print(f"éŒ¯èª¤: {str(e)}")
        send_to_telegram(f"âš ï¸ éŒ¯èª¤: {str(e)}")

    finally:
        # é—œé–‰ MySQL é€£æ¥
        if mysql_manager:
            mysql_manager.close()        
        
if __name__ == "__main__":
    main()